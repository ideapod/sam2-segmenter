# sam2-segmenter

Segment images for 3D reconstruction using a pipeline of [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO), [Mask2Former](https://github.com/facebookresearch/Mask2Former), and [SAM2](https://github.com/facebookresearch/sam2). Outputs are in the format expected by [SAM 3D Objects](https://github.com/facebookresearch/sam-3d-objects).

Designed for automated processing of historical photographs where manual annotation is not practical.

## Three segmentation pipelines

### 1. `segment_image_grounded.py` — Recommended
**Grounding DINO + SAM2** — Text-prompted object detection. You describe what's in the scene; DINO finds bounding boxes for each object; SAM2 produces pixel-accurate masks.

Best for: any image where you can describe the objects, especially historical scenes with unusual content.

Supports three prompt modes:
- **Fixed prompt file** — edit `prompts/historical_street_scene.txt` with one phrase per line
- **LLM-generated** — Claude analyses the image and generates prompts automatically (`--prompt-llm`)
- **Inline** — pass a prompt string directly (`--prompt "building . horse . cart"`)

### 2. `segment_image_panoptic.py` — Hybrid
**Mask2Former + SAM2 residual** — Panoptic segmentation handles known semantic classes (building, sky, road, person). SAM2 then runs on the uncovered area to catch objects outside the COCO vocabulary.

Best for: modern scenes where COCO classes cover most content, with some unusual foreground objects.

### 3. `segment_image.py` — Basic
**SAM2 automatic** — Boundary-based segmentation with no semantic understanding. Tends to over-segment complex scenes into hundreds of fragments.

Best for: simple images with clear, high-contrast objects.

---

## Output format

All pipelines produce the same output structure:

```
notebook/images/<image_name>/
    image.png           ← copy of input image
    0.png               ← mask for segment 0 (RGBA, alpha = mask)
    1.png               ← mask for segment 1
    ...
    _preview.png        ← colour-coded visualisation with legend
    _labels.txt         ← index, label, confidence, area per segment
    _llm_prompts.txt    ← (grounded LLM mode only) prompts generated by Claude
```

---

## Setup

### 1. Create the environment

```bash
mamba env create -f environment.yml
mamba activate sam2-segmenter
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Download a SAM2 checkpoint

```bash
mkdir -p checkpoints/sam2
wget https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt \
     -P checkpoints/sam2/
```

Other available checkpoints (smaller/faster):
- `sam2.1_hiera_base_plus.pt`
- `sam2.1_hiera_small.pt`
- `sam2.1_hiera_tiny.pt`

### 4. (Optional) HuggingFace login

Grounding DINO and Mask2Former weights download automatically from HuggingFace on first run (~700 MB and ~830 MB respectively). To avoid rate-limit warnings:

```bash
huggingface-cli login
```

### 5. (Optional) Anthropic API key for LLM prompts

```bash
export ANTHROPIC_API_KEY=sk-ant-...
# Or add to ~/.zshrc to persist
```

---

## Usage

### Grounded segmentation (recommended)

```bash
# Fixed prompt file (default: prompts/historical_street_scene.txt)
python segment_image_grounded.py --image /path/to/photo.jpg

# LLM-generated prompts — Claude analyses the image
python segment_image_grounded.py --image /path/to/photo.jpg --prompt-llm

# Inline prompt
python segment_image_grounded.py --image /path/to/photo.jpg \
    --prompt "building . horse . cart . person . sky"

# Custom output directory
python segment_image_grounded.py --image /path/to/photo.jpg \
    --output-dir /path/to/output/

# Filter out tiny segments (default 0.1% of image)
python segment_image_grounded.py --image /path/to/photo.jpg --min-area 0.01
```

#### Options

| Flag | Default | Description |
|------|---------|-------------|
| `--image` | required | Path to input image |
| `--prompt-file` | `prompts/historical_street_scene.txt` | Text file of phrases, one per line |
| `--prompt-llm` | off | Use Claude API to generate prompts from the image |
| `--prompt` | — | Inline dot-separated prompt string |
| `--box-threshold` | 0.30 | Grounding DINO detection confidence threshold |
| `--text-threshold` | 0.25 | Grounding DINO text similarity threshold |
| `--min-area` | 0.001 | Discard masks smaller than this fraction of the image |
| `--output-dir` | `notebook/images/<stem>/` | Output directory |
| `--checkpoint` | auto | SAM2 `.pt` checkpoint path |
| `--no-preview` | off | Skip saving preview PNG |

### Panoptic segmentation

```bash
python segment_image_panoptic.py --image /path/to/photo.jpg

# Skip SAM2 residual pass (panoptic only)
python segment_image_panoptic.py --image /path/to/photo.jpg --no-residual

# Denser residual sampling for small objects
python segment_image_panoptic.py --image /path/to/photo.jpg --residual-points 64
```

### Basic SAM2 segmentation

```bash
python segment_image.py --image /path/to/photo.jpg
python segment_image.py --image /path/to/photo.jpg --preset historical
python segment_image.py --image /path/to/photo.jpg --max-masks 20
```

---

## Editing the prompt file

`prompts/historical_street_scene.txt` contains one phrase per line. Lines starting with `#` are comments and are ignored. Edit it freely to suit your images:

```text
# My custom prompts
building
horse-drawn cart
person
sky
road
```

---

## Passing results to SAM 3D Objects

After segmentation, copy the output folder to your SAM 3D Objects instance and run:

```bash
python demo_multi3.py --image /path/to/notebook/images/<image_name>/image.png
```

---

## Platform support

| Platform | SAM2 | Grounding DINO | Mask2Former |
|----------|------|----------------|-------------|
| Apple Silicon (MPS) | ✓ | CPU fallback | ✓ |
| NVIDIA GPU (CUDA) | ✓ | ✓ | ✓ |
| CPU | ✓ (slow) | ✓ (slow) | ✓ (slow) |
