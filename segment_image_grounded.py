"""
Segment an image using Grounding DINO (text-prompted detection) + SAM2 (masking).

Unlike automatic segmentation, this approach uses natural language to find objects.
Grounding DINO detects bounding boxes for each described object, then SAM2 produces
a pixel-accurate mask for each box.

Two prompt modes:
  --prompt-file FILE   Use a text file of phrases (one per line, # = comment).
                       Default: prompts/historical_street_scene.txt
  --prompt-llm         Call Claude API to analyse the image and generate prompts
                       automatically. Saves generated prompts to _llm_prompts.txt
                       in the output directory for review.
  --prompt "foo . bar" Pass a prompt string directly on the command line.

Usage:
    # Fixed prompt file (default):
    python segment_image_grounded.py --image /path/to/image.jpg

    # Custom prompt file:
    python segment_image_grounded.py --image /path/to/image.jpg \\
        --prompt-file prompts/my_prompts.txt

    # LLM-generated prompts (requires ANTHROPIC_API_KEY env var):
    python segment_image_grounded.py --image /path/to/image.jpg --prompt-llm

    # Inline prompt:
    python segment_image_grounded.py --image /path/to/image.jpg \\
        --prompt "building . horse . cart . person"

Output:
    notebook/images/<image_name>/
        image.png               <- copy of input image
        0.png, 1.png, ...       <- one mask per detected object
        _preview.png            <- annotated visualisation with labels + boxes
        _labels.txt             <- index, label, confidence, area, box
        _llm_prompts.txt        <- (LLM mode only) prompts generated by Claude

Requirements:
    pip install groundingdino-py transformers accelerate scipy sam2 anthropic

    SAM2 checkpoint, e.g.:
        wget https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt \\
             -P checkpoints/sam2/

    Grounding DINO weights download automatically on first run (~700 MB).
"""

import os
import sys
import json
import base64
import argparse
import shutil
import numpy as np
from pathlib import Path
from PIL import Image


SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DEFAULT_PROMPT_FILE = os.path.join(SCRIPT_DIR, "prompts", "historical_street_scene.txt")
GDINO_MODEL = "IDEA-Research/grounding-dino-base"


# ---------------------------------------------------------------------------
# Device
# ---------------------------------------------------------------------------

def get_device():
    import torch
    if torch.cuda.is_available():   return "cuda"
    if torch.backends.mps.is_available(): return "mps"
    return "cpu"


# ---------------------------------------------------------------------------
# Prompt helpers
# ---------------------------------------------------------------------------

def load_prompt_file(path: str) -> str:
    """Read a prompt file, strip comments/blanks, return dot-joined string."""
    if not os.path.exists(path):
        print(f"✗ Prompt file not found: {path}")
        sys.exit(1)
    phrases = []
    with open(path) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith("#"):
                phrases.append(line)
    if not phrases:
        print(f"✗ No phrases found in {path}")
        sys.exit(1)
    prompt = " . ".join(phrases)
    print(f"✓ Loaded {len(phrases)} phrases from {path}")
    return prompt


def generate_prompt_llm(image_path: str, output_dir: str) -> str:
    """
    Call Claude claude-opus-4-5 with the image and ask it to list the objects present.
    Returns a dot-joined prompt string.
    Saves the full LLM response + prompts to _llm_prompts.txt in output_dir.
    """
    try:
        import anthropic
    except ImportError:
        print("✗ anthropic package not installed.  pip install anthropic")
        sys.exit(1)

    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        print("✗ ANTHROPIC_API_KEY environment variable not set.")
        sys.exit(1)

    print("Calling Claude to analyse image and generate prompts...")

    # Encode image as base64
    with open(image_path, "rb") as f:
        img_bytes = f.read()
    img_b64 = base64.standard_b64encode(img_bytes).decode("utf-8")

    # Detect media type
    suffix = Path(image_path).suffix.lower()
    media_type_map = {
        ".jpg": "image/jpeg", ".jpeg": "image/jpeg",
        ".png": "image/png", ".gif": "image/gif",
        ".webp": "image/webp", ".tiff": "image/tiff", ".tif": "image/tiff",
    }
    media_type = media_type_map.get(suffix, "image/jpeg")

    client = anthropic.Anthropic(api_key=api_key)

    system_prompt = """You are an expert at analysing historical photographs for computer vision segmentation.
Your task is to identify every distinct object or region in an image that should be segmented separately for 3D reconstruction.
Focus on objects that have physical depth and would benefit from individual 3D treatment.
Be specific but concise — use simple noun phrases that a vision model can match."""

    user_prompt = """Please analyse this historical photograph and list every distinct object or region that should be segmented separately for 3D scene reconstruction.

Return your response in this exact JSON format:
{
  "scene_description": "Brief description of the overall scene",
  "objects": [
    {"phrase": "short noun phrase", "reason": "why this should be separate"},
    ...
  ]
}

Guidelines:
- Each entry should be a simple noun phrase (e.g. "horse-drawn cart", "building facade", "person")
- Include background regions like "sky", "road", "dirt ground" as they are needed for scene completion
- If there are multiple instances of the same type (e.g. two carts), list the type once — the detector will find all instances
- Aim for 10-25 objects total
- Think about what a 3D reconstruction would need as separate elements"""

    response = client.messages.create(
        model="claude-opus-4-5",
        max_tokens=1024,
        system=system_prompt,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": media_type,
                            "data": img_b64,
                        },
                    },
                    {"type": "text", "text": user_prompt},
                ],
            }
        ],
    )

    raw_text = response.content[0].text
    print(f"✓ Claude response received")

    # Parse JSON from response
    try:
        # Handle markdown code blocks if present
        text = raw_text
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0].strip()
        elif "```" in text:
            text = text.split("```")[1].split("```")[0].strip()

        data = json.loads(text)
        scene_desc = data.get("scene_description", "")
        objects = data.get("objects", [])
        phrases = [o["phrase"] for o in objects if "phrase" in o]
    except (json.JSONDecodeError, KeyError) as e:
        print(f"⚠ Could not parse JSON response: {e}")
        print("  Falling back to line-by-line extraction")
        phrases = []
        for line in raw_text.split("\n"):
            line = line.strip().strip("-•*").strip()
            if line and 2 < len(line) < 50 and not line.startswith("{"):
                phrases.append(line)
        scene_desc = ""
        objects = [{"phrase": p, "reason": ""} for p in phrases]

    if not phrases:
        print("✗ No phrases extracted from LLM response.")
        print("Raw response:", raw_text)
        sys.exit(1)

    # Save to output file for review
    os.makedirs(output_dir, exist_ok=True)
    out_path = os.path.join(output_dir, "_llm_prompts.txt")
    with open(out_path, "w") as f:
        f.write(f"Scene description:\n{scene_desc}\n\n")
        f.write(f"Generated phrases ({len(phrases)}):\n")
        for o in objects:
            reason = o.get("reason", "")
            f.write(f"  - {o['phrase']}")
            if reason:
                f.write(f"  # {reason}")
            f.write("\n")
        f.write(f"\nDot-joined prompt:\n")
        f.write(" . ".join(phrases) + "\n")
        f.write(f"\nRaw LLM response:\n{raw_text}\n")

    print(f"✓ LLM prompts saved to: {out_path}")
    print(f"  Scene: {scene_desc}")
    print(f"  Phrases ({len(phrases)}): {' . '.join(phrases[:5])}{'...' if len(phrases) > 5 else ''}")

    return " . ".join(phrases)


# ---------------------------------------------------------------------------
# Grounding DINO detection
# ---------------------------------------------------------------------------

def run_grounding_dino(image_pil: Image.Image, prompt: str, device: str,
                       box_threshold: float = 0.30,
                       text_threshold: float = 0.25) -> list:
    """
    Run Grounding DINO on the image with the given text prompt.

    Returns list of dicts:
      { "label": str, "score": float, "box": [x0,y0,x1,y1] (pixel coords) }
    """
    try:
        from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
        import torch
    except ImportError:
        print("✗ transformers not installed.  pip install transformers accelerate")
        sys.exit(1)

    print(f"Loading Grounding DINO ({GDINO_MODEL})...")
    print("  (First run downloads ~700 MB — cached afterwards)")

    processor = AutoProcessor.from_pretrained(GDINO_MODEL)

    # Grounding DINO has known MPS issues with some ops — fall back to CPU if needed
    dino_device = device
    if device == "mps":
        print("  Note: Grounding DINO running on CPU (MPS not fully supported for this model)")
        dino_device = "cpu"

    model = AutoModelForZeroShotObjectDetection.from_pretrained(GDINO_MODEL)
    model = model.to(dino_device)
    model.eval()
    print("✓ Grounding DINO loaded")

    # Grounding DINO requires the prompt to end with a period
    text = prompt.strip()
    if not text.endswith("."):
        text = text + "."

    print(f"Running detection with prompt: {text[:80]}{'...' if len(text)>80 else ''}")

    import torch
    with torch.no_grad():
        inputs = processor(images=image_pil, text=text, return_tensors="pt")
        inputs = {k: v.to(dino_device) for k, v in inputs.items()}
        outputs = model(**inputs)

    # Post-process
    results = processor.post_process_grounded_object_detection(
        outputs,
        inputs["input_ids"],
        box_threshold=box_threshold,
        text_threshold=text_threshold,
        target_sizes=[image_pil.size[::-1]],   # (H, W)
    )[0]

    detections = []
    w, h = image_pil.size
    for score, label, box in zip(
        results["scores"].tolist(),
        results["labels"],
        results["boxes"].tolist(),
    ):
        # Clamp to image bounds
        x0 = max(0, int(box[0]))
        y0 = max(0, int(box[1]))
        x1 = min(w, int(box[2]))
        y1 = min(h, int(box[3]))
        if x1 <= x0 or y1 <= y0:
            continue
        detections.append({
            "label": label,
            "score": score,
            "box":   [x0, y0, x1, y1],
        })

    # Sort by box area descending
    detections.sort(key=lambda d: (d["box"][2]-d["box"][0]) * (d["box"][3]-d["box"][1]), reverse=True)
    print(f"✓ Detected {len(detections)} objects")
    for i, d in enumerate(detections):
        b = d["box"]
        print(f"  [{i:3d}] {d['label']:30s}  score={d['score']:.2f}  box=[{b[0]},{b[1]},{b[2]},{b[3]}]")

    return detections


# ---------------------------------------------------------------------------
# SAM2 masking from boxes
# ---------------------------------------------------------------------------

def load_sam2_predictor(checkpoint_path=None, model_cfg=None, device="cpu"):
    try:
        from sam2.build_sam import build_sam2
        from sam2.sam2_image_predictor import SAM2ImagePredictor
    except ImportError:
        print("✗ SAM2 not installed.  pip install sam2")
        sys.exit(1)

    if checkpoint_path is None:
        candidates = [
            "checkpoints/sam2/sam2.1_hiera_large.pt",
            "checkpoints/sam2/sam2.1_hiera_base_plus.pt",
            "checkpoints/sam2/sam2.1_hiera_small.pt",
            "checkpoints/sam2/sam2.1_hiera_tiny.pt",
            "checkpoints/sam2/sam2_hiera_large.pt",
        ]
        for c in candidates:
            if os.path.exists(c):
                checkpoint_path = c
                break
        if checkpoint_path is None:
            print("✗ No SAM2 checkpoint found in checkpoints/sam2/")
            sys.exit(1)

    if model_cfg is None:
        name = Path(checkpoint_path).stem
        if "large"     in name: model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"
        elif "base_plus" in name: model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"
        elif "small"   in name: model_cfg = "configs/sam2.1/sam2.1_hiera_s.yaml"
        else:                     model_cfg = "configs/sam2.1/sam2.1_hiera_t.yaml"

    print(f"Loading SAM2 predictor from: {checkpoint_path}")
    import torch
    sam2 = build_sam2(model_cfg, checkpoint_path, device=device)
    predictor = SAM2ImagePredictor(sam2)
    print("✓ SAM2 predictor loaded")
    return predictor


def mask_detections_with_sam2(predictor, image_rgb: np.ndarray,
                               detections: list) -> list:
    """
    For each detection box, use SAM2 to produce a pixel mask.
    Returns detections with "mask" (H,W bool) and "area_frac" added.
    """
    import torch
    import numpy as np

    h, w = image_rgb.shape[:2]
    total = h * w
    results = []

    print(f"Masking {len(detections)} detections with SAM2...")
    for i, det in enumerate(detections):
        pct = (i + 1) / len(detections)
        bar = ("█" * int(pct * 20)).ljust(20)
        print(f"\r  [{bar}] {i+1}/{len(detections)}  {det['label'][:25]}", end="", flush=True)

        box = np.array(det["box"], dtype=np.float32)   # [x0,y0,x1,y1]

        with torch.no_grad():
            masks, scores, _ = predictor.predict(
                box=box,
                multimask_output=True,
            )

        # Pick highest-scoring mask
        best = int(np.argmax(scores))
        mask = masks[best].astype(bool)

        results.append({
            **det,
            "mask":      mask,
            "area_frac": mask.sum() / total,
        })

    print("\n✓ SAM2 masking complete")
    return results


# ---------------------------------------------------------------------------
# Deduplicate (NMS by mask IoU)
# ---------------------------------------------------------------------------

def deduplicate(segments: list, iou_threshold: float = 0.6) -> list:
    """Greedy NMS — keeps higher-confidence detection when two overlap heavily."""
    # Sort by score descending so we keep the more confident one
    segments = sorted(segments, key=lambda s: s["score"], reverse=True)
    kept = []
    for seg in segments:
        m = seg["mask"]
        suppress = False
        for k in kept:
            inter = (m & k["mask"]).sum()
            union = (m | k["mask"]).sum()
            if union > 0 and inter / union > iou_threshold:
                suppress = True
                break
        if not suppress:
            kept.append(seg)
    # Re-sort by area for consistent output ordering
    kept.sort(key=lambda s: s["area_frac"], reverse=True)
    return kept


# ---------------------------------------------------------------------------
# Save outputs
# ---------------------------------------------------------------------------

def save_output(image_path: str, image_rgb: np.ndarray,
                segments: list, output_dir: str):
    os.makedirs(output_dir, exist_ok=True)

    dest = os.path.join(output_dir, "image.png")
    shutil.copy2(image_path, dest)
    print(f"✓ Image copied to: {dest}")

    h, w = image_rgb.shape[:2]
    label_lines = ["index\tlabel\tscore\tarea\tbox"]
    total = len(segments)

    for idx, seg in enumerate(segments):
        rgba = np.zeros((h, w, 4), dtype=np.uint8)
        rgba[..., :3] = image_rgb
        rgba[..., 3]  = seg["mask"].astype(np.uint8) * 255
        Image.fromarray(rgba, mode="RGBA").save(
            os.path.join(output_dir, f"{idx}.png")
        )
        b = seg["box"]
        label_lines.append(
            f"{idx}\t{seg['label']}\t{seg['score']:.2f}\t"
            f"{seg['area_frac']*100:.1f}%\t[{b[0]},{b[1]},{b[2]},{b[3]}]"
        )

        pct = (idx + 1) / total
        bar = ("█" * int(pct * 20)).ljust(20)
        print(f"\r  Saving masks: [{bar}] {idx+1}/{total}", end="", flush=True)

    print()

    labels_path = os.path.join(output_dir, "_labels.txt")
    with open(labels_path, "w") as f:
        f.write("\n".join(label_lines))
    print(f"✓ {len(segments)} masks saved to: {output_dir}")
    print(f"✓ Labels: {labels_path}")


def save_preview(image_rgb: np.ndarray, segments: list, out_path: str):
    import matplotlib.pyplot as plt
    import matplotlib.patches as mpatches
    from matplotlib.patches import Rectangle

    fig, ax = plt.subplots(1, 1, figsize=(16, 10))
    ax.imshow(image_rgb)

    np.random.seed(42)
    colors  = np.random.rand(len(segments), 3)
    patches = []

    for i, seg in enumerate(segments):
        # Mask overlay
        overlay = np.zeros((*seg["mask"].shape, 4))
        overlay[..., :3] = colors[i]
        overlay[..., 3]  = seg["mask"] * 0.45
        ax.imshow(overlay)

        # Bounding box
        b = seg["box"]
        rect = Rectangle(
            (b[0], b[1]), b[2]-b[0], b[3]-b[1],
            linewidth=1.5, edgecolor=colors[i], facecolor="none", alpha=0.9
        )
        ax.add_patch(rect)

        # Label text at top-left of box
        ax.text(
            b[0]+2, b[1]+2, f"{i}:{seg['label']}",
            fontsize=6, color="white",
            bbox=dict(facecolor=colors[i], alpha=0.7, pad=1, edgecolor="none"),
            verticalalignment="top",
        )

        patches.append(mpatches.Patch(
            color=colors[i],
            label=f"{i}: {seg['label']} ({seg['area_frac']*100:.1f}%, {seg['score']:.2f})"
        ))

    ax.axis("off")
    ax.set_title(f"{len(segments)} grounded segments (Grounding DINO + SAM2)", fontsize=13)
    ax.legend(handles=patches[:25], loc="upper right", fontsize=7, framealpha=0.7)
    plt.tight_layout()
    plt.savefig(out_path, dpi=130, bbox_inches="tight")
    plt.close()
    print(f"✓ Preview saved to: {out_path}")


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description="Grounding DINO + SAM2 text-prompted segmentation for demo_multi3.py"
    )
    parser.add_argument("--image",        required=True,  help="Path to input image")
    parser.add_argument("--checkpoint",   default=None,   help="SAM2 .pt checkpoint path")
    parser.add_argument("--model-cfg",    default=None,   help="SAM2 model config YAML")
    parser.add_argument("--output-dir",   default=None,   help="Output dir (default: notebook/images/<stem>/)")

    # Prompt modes (mutually exclusive)
    prompt_group = parser.add_mutually_exclusive_group()
    prompt_group.add_argument(
        "--prompt-file", default=None,
        help=f"Text file of phrases, one per line (default: {DEFAULT_PROMPT_FILE})",
    )
    prompt_group.add_argument(
        "--prompt-llm", action="store_true",
        help="Use Claude API to generate prompts from the image (requires ANTHROPIC_API_KEY)",
    )
    prompt_group.add_argument(
        "--prompt", default=None,
        help='Inline dot-separated prompt, e.g. "building . horse . cart"',
    )

    parser.add_argument(
        "--box-threshold", type=float, default=0.30,
        help="Grounding DINO detection confidence threshold (default: 0.30)",
    )
    parser.add_argument(
        "--text-threshold", type=float, default=0.25,
        help="Grounding DINO text similarity threshold (default: 0.25)",
    )
    parser.add_argument(
        "--min-area", type=float, default=0.001,
        help="Discard masks smaller than this fraction of image (default: 0.001 = 0.1%%)",
    )
    parser.add_argument("--no-preview",  action="store_true", help="Skip preview PNG")
    args = parser.parse_args()

    image_path = os.path.abspath(args.image)
    if not os.path.exists(image_path):
        print(f"✗ Image not found: {image_path}")
        sys.exit(1)

    image_stem = Path(image_path).stem
    output_dir = (
        os.path.abspath(args.output_dir) if args.output_dir
        else os.path.join(SCRIPT_DIR, "notebook", "images", image_stem)
    )

    device = get_device()
    print(f"\nInput    : {image_path}")
    print(f"Output   : {output_dir}")
    print(f"Device   : {device}")
    print(f"Min area : {args.min_area*100:.2f}%\n")

    image_pil = Image.open(image_path).convert("RGB")
    image_rgb = np.array(image_pil)
    print(f"✓ Image loaded: {image_rgb.shape[1]}x{image_rgb.shape[0]}")

    # ── Resolve prompt ────────────────────────────────────────────────────
    if args.prompt:
        prompt = args.prompt
        print(f"✓ Using inline prompt: {prompt[:80]}")
    elif args.prompt_llm:
        prompt = generate_prompt_llm(image_path, output_dir)
    else:
        prompt_file = args.prompt_file or DEFAULT_PROMPT_FILE
        prompt = load_prompt_file(prompt_file)

    # ── Grounding DINO detection ──────────────────────────────────────────
    detections = run_grounding_dino(
        image_pil, prompt, device,
        box_threshold=args.box_threshold,
        text_threshold=args.text_threshold,
    )

    if not detections:
        print("✗ No objects detected. Try lowering --box-threshold or adjusting your prompt.")
        sys.exit(1)

    # ── SAM2 masking ──────────────────────────────────────────────────────
    predictor = load_sam2_predictor(args.checkpoint, args.model_cfg, device)
    predictor.set_image(image_rgb)
    segments = mask_detections_with_sam2(predictor, image_rgb, detections)

    # Filter tiny masks
    before = len(segments)
    segments = [s for s in segments if s["area_frac"] >= args.min_area]
    if before != len(segments):
        print(f"  Filtered {before - len(segments)} tiny masks → {len(segments)} kept")

    # Deduplicate overlapping detections
    before = len(segments)
    segments = deduplicate(segments, iou_threshold=0.6)
    if before != len(segments):
        print(f"  Deduplicated {before - len(segments)} overlapping masks → {len(segments)} kept")

    if not segments:
        print("✗ No segments remaining after filtering.")
        sys.exit(1)

    # ── Save ──────────────────────────────────────────────────────────────
    save_output(image_path, image_rgb, segments, output_dir)

    if not args.no_preview:
        save_preview(image_rgb, segments, os.path.join(output_dir, "_preview.png"))

    print(f"\n{'='*60}")
    print("✓ Done! Run the 3D model with:")
    print(f"    python demo_multi3.py --image {os.path.join(output_dir, 'image.png')}")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
